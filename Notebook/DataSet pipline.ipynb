{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff92074-952d-43e8-87fa-26d5880cf10a",
   "metadata": {},
   "source": [
    "# *Create DataSet Pipline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9450bc-1af2-4b96-bca8-9349de1d32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_pipeline(source_dir, base_dir, image_size=(224, 224), val_ratio=0.20):\n",
    "    import os\n",
    "    import shutil\n",
    "    import cv2\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \"\"\"\n",
    "    Augments train & test images separately.\n",
    "    Splits only the train set into train and validation sets.\n",
    "    \"\"\"\n",
    "    print(\"Starting Data Engineering Pipeline...\")\n",
    "\n",
    "    # Directories\n",
    "    augmented_train_dir = os.path.join(base_dir, '01_augmented_train')\n",
    "    augmented_test_dir = os.path.join(base_dir, '02_augmented_test')\n",
    "    train_dir = os.path.join(base_dir, '03_train')\n",
    "    validation_dir = os.path.join(base_dir, '04_validation')\n",
    "    test_dir = os.path.join(base_dir, '05_test')\n",
    "\n",
    "    # Remove old output\n",
    "    if os.path.exists(base_dir):\n",
    "        print(f\"Removing old base directory: {base_dir}\")\n",
    "        shutil.rmtree(base_dir)\n",
    "    os.makedirs(augmented_train_dir, exist_ok=True)\n",
    "    os.makedirs(augmented_test_dir, exist_ok=True)\n",
    "\n",
    "    def augment_and_save(src_split_dir, out_split_dir):\n",
    "        \"\"\"Resize + rotate + flip images and save them.\"\"\"\n",
    "        class_labels = [d for d in os.listdir(src_split_dir) if os.path.isdir(os.path.join(src_split_dir, d))]\n",
    "        print(f\"Found classes in {src_split_dir}: {class_labels}\")\n",
    "\n",
    "        for class_name in class_labels:\n",
    "            src_class_dir = os.path.join(src_split_dir, class_name)\n",
    "            out_class_dir = os.path.join(out_split_dir, class_name)\n",
    "            os.makedirs(out_class_dir, exist_ok=True)\n",
    "\n",
    "            for filename in os.listdir(src_class_dir):\n",
    "                img_path = os.path.join(src_class_dir, filename)\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Could not read image: {img_path}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                resized = cv2.resize(img, (image_size[1], image_size[0]))\n",
    "                rotated = cv2.rotate(resized, cv2.ROTATE_90_CLOCKWISE)\n",
    "                flipped = cv2.flip(resized, 0)\n",
    "\n",
    "                base_name, ext = os.path.splitext(filename)\n",
    "                cv2.imwrite(os.path.join(out_class_dir, f\"{base_name}_orig{ext}\"), resized)\n",
    "                cv2.imwrite(os.path.join(out_class_dir, f\"{base_name}_rot{ext}\"), rotated)\n",
    "                cv2.imwrite(os.path.join(out_class_dir, f\"{base_name}_flip{ext}\"), flipped)\n",
    "\n",
    "        return class_labels\n",
    "\n",
    "    # Augment train and test sets separately\n",
    "    print(\"Augmenting TRAIN set...\")\n",
    "    train_labels = augment_and_save(os.path.join(source_dir, 'train'), augmented_train_dir)\n",
    "    print(\"Augmenting TEST set...\")\n",
    "    _ = augment_and_save(os.path.join(source_dir, 'test'), augmented_test_dir)\n",
    "\n",
    "    # Collect augmented train files for splitting\n",
    "    all_train_files = []\n",
    "    all_train_labels = []\n",
    "    for class_name in train_labels:\n",
    "        class_dir = os.path.join(augmented_train_dir, class_name)\n",
    "        for fname in os.listdir(class_dir):\n",
    "            all_train_files.append(os.path.join(class_dir, fname))\n",
    "            all_train_labels.append(class_name)\n",
    "\n",
    "    # Split train into train/validation\n",
    "    train_files, val_files, train_labels_split, val_labels_split = train_test_split(\n",
    "        all_train_files, all_train_labels,\n",
    "        test_size=val_ratio,\n",
    "        random_state=42,\n",
    "        stratify=all_train_labels\n",
    "    )\n",
    "\n",
    "    def copy_files(file_list, destination_dir):\n",
    "        for file_path in file_list:\n",
    "            cls = os.path.basename(os.path.dirname(file_path))\n",
    "            dest_class_dir = os.path.join(destination_dir, cls)\n",
    "            os.makedirs(dest_class_dir, exist_ok=True)\n",
    "            shutil.copy(file_path, dest_class_dir)\n",
    "\n",
    "    # Create final dirs\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Copying TRAIN files...\")\n",
    "    copy_files(train_files, train_dir)\n",
    "    print(\"Copying VALIDATION files...\")\n",
    "    copy_files(val_files, validation_dir)\n",
    "    print(\"Copying TEST files...\")\n",
    "    for class_name in os.listdir(augmented_test_dir):\n",
    "        class_src_dir = os.path.join(augmented_test_dir, class_name)\n",
    "        for fname in os.listdir(class_src_dir):\n",
    "            dest_class_dir = os.path.join(test_dir, class_name)\n",
    "            os.makedirs(dest_class_dir, exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_src_dir, fname), dest_class_dir)\n",
    "\n",
    "    print(\"Pipeline finished successfully.\")\n",
    "    print(f\"Train images: {len(train_files)}\")\n",
    "    print(f\"Validation images: {len(val_files)}\")\n",
    "    print(f\"Test images: sum([len(files) for files in test_dir])\")\n",
    "\n",
    "    return {\n",
    "        'augmented_train_dir': augmented_train_dir,\n",
    "        'augmented_test_dir': augmented_test_dir,\n",
    "        'train_dir': train_dir,\n",
    "        'validation_dir': validation_dir,\n",
    "        'test_dir': test_dir,\n",
    "        'class_labels': train_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ce2b23-43d7-414b-a2f2-910f04c34443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Data Engineering Pipeline...\n",
      "Removing old base directory: F:\\Rough preprocess image\n",
      "Augmenting TRAIN set...\n",
      "Found classes in F:\\brain tumor classification final year project\\MRI_Orignal Data\\train: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "Augmenting TEST set...\n",
      "Found classes in F:\\brain tumor classification final year project\\MRI_Orignal Data\\test: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
      "Copying TRAIN files...\n",
      "Copying VALIDATION files...\n",
      "Copying TEST files...\n",
      "Pipeline finished successfully.\n",
      "Train images: 6888\n",
      "Validation images: 1722\n",
      "Test images: sum([len(files) for files in test_dir])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'augmented_train_dir': 'F:\\\\Rough preprocess image\\\\01_augmented_train',\n",
       " 'augmented_test_dir': 'F:\\\\Rough preprocess image\\\\02_augmented_test',\n",
       " 'train_dir': 'F:\\\\Rough preprocess image\\\\03_train',\n",
       " 'validation_dir': 'F:\\\\Rough preprocess image\\\\04_validation',\n",
       " 'test_dir': 'F:\\\\Rough preprocess image\\\\05_test',\n",
       " 'class_labels': ['glioma_tumor',\n",
       "  'meningioma_tumor',\n",
       "  'no_tumor',\n",
       "  'pituitary_tumor']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir, base_dir = r\"F:\\brain tumor classification final year project\\MRI_Orignal Data\",r\"F:\\Rough preprocess image\"\n",
    "create_dataset_pipeline(source_dir,base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931327bd-fb66-49de-8145-2b06bcdd0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Tqdm when the datset pipline is executed it show the progreess which loop is executed and which loop is not executed aslo show the total number of image after the augmentatio and total number of image before the augmentaton also print some other necessary infomation if needed , in this function another problem is that it duplicate the data set first it store the augment train image , and than it seprate this augment train into three directry train ,test and validation , remove this duplication adjust the function that only store the the train ,validation and final  test set not train_augment,test_augment, train,test and validation you see in the screenshut "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
