{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "616f5dac",
   "metadata": {},
   "source": [
    "# **Create model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "def evaluate_model(model, test_generator, model_name, class_names, save_dir):\n",
    "    \"\"\"Evaluate the model on test data and generate metrics and visualizations\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_probs = model.predict(test_generator)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    test_accuracy = np.mean(y_pred == y_true)\n",
    "    print(f\"Test Accuracy for {model_name}: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    print(f\"Classification Report for {model_name}:\\n{report}\")\n",
    "     # âœ… Save classification report\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    report_path = os.path.join(save_dir, f\"classification_report_{model_name}.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"Classification report saved to {report_path}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'confusion_matrix_{model_name}.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    # ROC Curve (for binary or multi-class classification)\n",
    "    if len(class_names) == 2:\n",
    "        # Binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'roc_curve_{model_name}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {save_path}\")\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        # Binarize the output\n",
    "        y_true_bin = tf.keras.utils.to_categorical(y_true, len(class_names))\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(len(class_names)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_probs.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "        # Plot ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label=f'micro-average ROC curve (area = {roc_auc[\"micro\"]:.2f})',\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "        \n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'pink', 'brown', 'gray', 'olive'])\n",
    "        for i, color in zip(range(len(class_names)), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                     label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'roc_curve_{model_name}.png')\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"ROC curve saved to {save_path}\")\n",
    "    \n",
    "    return test_accuracy, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54513428",
   "metadata": {},
   "source": [
    "# **Compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28668fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(model_scores, save_dir):\n",
    "    \"\"\"Plot a comparison of model accuracies\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    models = list(model_scores.keys())\n",
    "    scores = list(model_scores.values())\n",
    "    \n",
    "    bars = plt.bar(models, scores, color=['blue', 'green', 'red', 'purple', 'orange'])\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)  # Assuming accuracy is between 0 and 1\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, 'model_comparison.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Model comparison plot saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
